; Copyright (C) 2024 Intel Corporation
; SPDX-License-Identifier: BSD-3-Clause

; RUN: %tinytc-oc < %s | filecheck %s
func @axpby_atomic_store(%alpha: f64, %A: memref<f64x2>, %B: memref<f64x2>) {
    axpby.n.atomic %alpha, %A, 0.0, %B : f64, memref<f64x2>, f64, memref<f64x2>
; CHECK:      global double* b = B + (blck + m) * 1;
; CHECK-NEXT: atomic_store_explicit((global volatile atomic_double*) b, alpha * A[(blck + m) * 1], memory_order_relaxed, memory_scope_work_group);
}

func @axpby_atomic_add(%alpha: f32, %A: memref<f32x2x2>, %B: memref<f32x2x2>) {
    axpby.n.atomic %alpha, %A, 1.0, %B : f32, memref<f32x2x2>, f32, memref<f32x2x2>
; CHECK:      global float* b = Bb + (blck1 + m) * 1;
; CHECK-NEXT: atomic_fetch_add_explicit((global volatile atomic_float*) b, alpha * Ab[(blck1 + m) * 1], memory_order_relaxed, memory_scope_work_group);
}

func @axpby_atomic_general(%alpha: f32, %A: memref<f32>, %B: memref<f32>) {
    axpby.n.atomic %alpha, %A, 3.14, %B : f32, memref<f32>, f32, memref<f32>
; CHECK:      float expected = *B;
; CHECK-NEXT: float desired;
; CHECK-NEXT: do {
; CHECK-NEXT:     desired = alpha * A[0] + 0x1.91eb851eb851fp+1f * expected;
; CHECK-NEXT: } while (atomic_compare_exchange_strong_explicit((global volatile atomic_float*) B, &expected, desired, memory_order_relaxed, memory_order_relaxed, memory_scope_work_group));
}

func @gemm_atomic(%A: memref<f32x32x2>, %B: memref<f32x2x2>, %C: memref<f32x32x2>) {
    gemm.n.n.atomic 1.0, %A, %B, 1.0, %C
        : f32, memref<f32x32x2>, memref<f32x2x2>, f32, memref<f32x32x2>
; CHECK: atomic_fetch_add_explicit((global volatile atomic_float*) (Cb + get_sub_group_local_id()), c[n], memory_order_relaxed, memory_scope_work_group);
}

func @ger_atomic(%A: memref<f32x32>, %B: memref<f32x2>, %C: memref<f32x32x2>) {
    ger.atomic 1.0, %A, %B, 1.0, %C
        : f32, memref<f32x32>, memref<f32x2>, f32, memref<f32x32x2>
; CHECK:      global float* c = Cb + (blck1 + m) * 1;
; CHECK-NEXT: atomic_fetch_add_explicit((global volatile atomic_float*) c, 0x1p+0f * A[(blck1 + m) * 1] * b, memory_order_relaxed, memory_scope_work_group);
}

func @hadamard_atomic(%A: memref<f32x32>, %B: memref<f32x32>, %C: memref<f32x32>) {
    hadamard.atomic 1.0, %A, %B, 1.0, %C
        : f32, memref<f32x32>, memref<f32x32>, f32, memref<f32x32>
; CHECK:      global float* c = C + (blck + m) * 1;
; CHECK-NEXT: atomic_fetch_add_explicit((global volatile atomic_float*) c, 0x1p+0f * A[(blck + m) * 1] * B[(blck + m) * 1], memory_order_relaxed, memory_scope_work_group);
}

func @sum_atomic(%A: memref<f32x32>, %B: memref<f32>) {
    sum.n.atomic 1.0, %A, 1.0, %B : f32, memref<f32x32>, f32, memref<f32>
; CHECK:      float sum = work_group_reduce_add(acc);
; CHECK-NEXT: if (get_sub_group_id() == 0 && get_sub_group_local_id() == 0) {
; CHECK-NEXT:     atomic_fetch_add_explicit((global volatile atomic_float*) B, 0x1p+0f * sum, memory_order_relaxed, memory_scope_work_group);
; CHECK-NEXT: }
}

func @sum_atomic_matrix(%A: memref<f32x32x4>, %B: memref<f32x32>) {
    sum.n.atomic 1.0, %A, 1.0, %B : f32, memref<f32x32x4>, f32, memref<f32x32>
; CHECK:      global float* b = B + (blck + m) * 1;
; CHECK-NEXT: atomic_fetch_add_explicit((global volatile atomic_float*) b, 0x1p+0f * acc, memory_order_relaxed, memory_scope_work_group);
}
